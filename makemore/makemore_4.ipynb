{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77981a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt # for figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484db22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read words\n",
    "words = open('names.txt','r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49fdbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab and mappings to/from ints\n",
    "vocab = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(vocab)}\n",
    "stoi['.']=0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c3eaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182435, 3]) torch.Size([182435])\n",
      "torch.Size([22840, 3]) torch.Size([22840])\n",
      "torch.Size([22871, 3]) torch.Size([22871])\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "\n",
    "block_size = 3 # ctx length\n",
    "\n",
    "def build_dataset(words):\n",
    "    X,Y = [],[]\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0]*block_size\n",
    "        for ch in w+'.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X,Y = torch.tensor(X), torch.tensor(Y)\n",
    "    print(X.shape,Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "random.shuffle(words)\n",
    "n = len(words)\n",
    "n1,n2  = int(0.8*n),int(0.9*n)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8714d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility func to compare manual grads to pytorch grads\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt==t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt-t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b1d548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # dimensionality of char embedding vectors\n",
    "n_hidden = 64 # num of hidden neurons\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((vocab_size,n_embd), generator = g)\n",
    "#Layer 1\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden), generator=g) * (5/3) / ((n_embd*block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1 # just to check grad\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# batchNorm params\n",
    "bngain = torch.randn((1,n_hidden), generator =g ) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1,n_hidden))*0.1\n",
    "\n",
    "# non standard init for just check grads as zero vals can mask incorect gradients\n",
    "\n",
    "parameters = [C,W1,b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # total params\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e75182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # shorter var size for convenice\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0],(batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd1b81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4609, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, chunked into smaller steps to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed char into vectors\n",
    "embcat = emb.view(emb.shape[0],-1) # concat \n",
    "# linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre activation\n",
    "# batch norm layer\n",
    "bnmeani = 1/n*hprebn.sum(0,keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0,keepdim=True) # bessel's correction n-1 not n\n",
    "bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "bnraw = bndiff*bnvar_inv\n",
    "hpreact = bngain*bnraw + bnbias\n",
    "# non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer tanh\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same F.cross_entropy)\n",
    "logit_maxes = logits.max(1,keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1,keepdim=True) # 32x37 => 32x1\n",
    "counts_sum_inv = counts_sum**-1 # doing 1/count_sum wouldn't be exact for backprop\n",
    "probs = counts * counts_sum_inv # 32x27, 32x1\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n),Yb].mean()\n",
    "\n",
    "# pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad=None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "         norm_logits, logit_maxes, logits, h, hpreact, bnraw, \n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d58c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 64]), torch.Size([64, 27]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, h.shape, W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "999f142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnbias         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbngain         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnraw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnvar_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnvar          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbndiff2        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbndiff         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnmeani        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhprebn         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dembcat         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "demb            | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dC              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Ex 1 : do the whole backprop thing manually \n",
    "\n",
    "# loss = -logprobs[range(n),Yb].mean()\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n),Yb] = -1.0/n\n",
    "\n",
    "dlogprobs_probs = probs**-1\n",
    "dprobs = dlogprobs_probs * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1,keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = - (counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += dcounts_sum.broadcast_to(counts.shape)\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1,keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices,num_classes = logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# logits = h @ W2 + b2\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0) # broadcasted across rows (replicated vertically)\n",
    "\n",
    "# h = torch.tanh(hpreact)\n",
    "dhpreact = (1.0-h**2)*dh\n",
    "\n",
    "# hpreact = bngain*bnraw + bnbias\n",
    "dbngain = (bnraw*dhpreact).sum(0,keepdim=True)\n",
    "dbnraw = bngain*dhpreact\n",
    "dbnbias = dhpreact.sum(0)\n",
    "\n",
    "# bnraw = bndiff*bnvar_inv\n",
    "dbndiff = bnvar_inv*dbnraw\n",
    "dbnvar_inv = (bndiff*dbnraw).sum(0,keepdim=True)\n",
    "\n",
    "# bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "dbnvar = -0.5 * (bnvar+1e-5)**(-1.5) * dbnvar_inv\n",
    "\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0,keepdim=True)\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff2 = (1.0/(n-1)) * dbnvar.broadcast_to(bndiff2.shape)\n",
    "\n",
    "# bndiff2 = bndiff**2\n",
    "dbndiff += 2* bndiff * dbndiff2\n",
    "\n",
    "# bndiff = hprebn - bnmeani\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0,keepdim=True)\n",
    "\n",
    "# bnmeani = 1/n*hprebn.sum(0,keepdim=True)\n",
    "dhprebn += (1.0/n)*dbnmeani.broadcast_to(hprebn.shape)\n",
    "\n",
    "# hprebn = embcat @ W1 + b1\n",
    "db1 = dhprebn.sum(0)\n",
    "dW1 = embcat.T @ dhprebn\n",
    "dembcat = dhprebn @ W1.T\n",
    "\n",
    "# embcat = emb.view(emb.shape[0],-1)\n",
    "demb = dembcat.reshape(emb.shape)\n",
    "\n",
    "# emb = C[Xb]\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix]+=demb[k,j]\n",
    "\n",
    "\n",
    "cmp('logprobs',dlogprobs, logprobs)\n",
    "cmp('probs',dprobs, probs)\n",
    "cmp('counts_sum_inv',dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum',dcounts_sum, counts_sum)\n",
    "cmp('counts',dcounts, counts)\n",
    "cmp('norm_logits',dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes',dlogit_maxes, logit_maxes)\n",
    "cmp('logits',dlogits,logits)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dh',dh,h)\n",
    "cmp('dW2',dW2,W2)\n",
    "cmp('dhpreact',dhpreact,hpreact)\n",
    "cmp('dbnbias',dbnbias,bnbias)\n",
    "cmp('dbngain',dbngain,bngain)\n",
    "cmp('dbnraw',dbnraw,bnraw)\n",
    "cmp('dbnvar_inv',dbnvar_inv,bnvar_inv)\n",
    "cmp('dbnvar',dbnvar,bnvar)\n",
    "cmp('dbndiff2',dbndiff2,bndiff2)\n",
    "cmp('dbndiff',dbndiff,bndiff)\n",
    "cmp('dbnmeani',dbnmeani,bnmeani)\n",
    "cmp('dhprebn',dhprebn,hprebn)\n",
    "cmp('db1',db1,b1)\n",
    "cmp('dW1',dW1,W1)\n",
    "cmp('dembcat',dembcat,embcat)\n",
    "cmp('demb',demb,emb)\n",
    "cmp('dC',dC, C)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705e2ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 30]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, embcat.shape, W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5df3920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([27, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, C.shape, Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e280606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.460947036743164 3.460947036743164 diff:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Ex 2: backprop throgh cross_entropy in one go\n",
    "# take derivative of loss wrt logits\n",
    "\n",
    "# forwad pass \n",
    "# before\n",
    "# logit_maxes = logits.max(1,keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1,keepdim=True) # 32x37 => 32x1\n",
    "# counts_sum_inv = counts_sum**-1 # doing 1/count_sum wouldn't be exact for backprop\n",
    "# probs = counts * counts_sum_inv # 32x27, 32x1\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n),Yb].mean()\n",
    "\n",
    "# Now:\n",
    "loss_fast = F.cross_entropy(logits,Yb)\n",
    "print(loss_fast.item(), loss.item(), 'diff: ', (loss_fast-loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "062880dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "dlogits = F.softmax(logits,1)\n",
    "dlogits[range(n),Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('dlogits', dlogits,logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83d80f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is dlogits?\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5477f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm layer but all in one go\n",
    "# look at exp for batchnorm and take deriv wrt its input\n",
    "\n",
    "# forward pass\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0,keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0,keepdim=True) # bessel's correction n-1 not n\n",
    "# bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "# bnraw = bndiff*bnvar_inv\n",
    "# hpreact = bngain*bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0,keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max_diff: ', (hpreact_fast-hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "756838da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "# dbngain = (bnraw*dhpreact).sum(0,keepdim=True)\n",
    "# dbnraw = bngain*dhpreact\n",
    "# dbnbias = dhpreact.sum(0)\n",
    "\n",
    "# # bnraw = bndiff*bnvar_inv\n",
    "# dbndiff = bnvar_inv*dbnraw\n",
    "# dbnvar_inv = (bndiff*dbnraw).sum(0,keepdim=True)\n",
    "\n",
    "# # bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "# dbnvar = -0.5 * (bnvar+1e-5)**(-1.5) * dbnvar_inv\n",
    "\n",
    "# # bnvar = 1/(n-1)*(bndiff2).sum(0,keepdim=True)\n",
    "# # dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff2 = (1.0/(n-1)) * dbnvar.broadcast_to(bndiff2.shape)\n",
    "\n",
    "# # bndiff2 = bndiff**2\n",
    "# dbndiff += 2* bndiff * dbndiff2\n",
    "\n",
    "# # bndiff = hprebn - bnmeani\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0,keepdim=True)\n",
    "\n",
    "# # bnmeani = 1/n*hprebn.sum(0,keepdim=True)\n",
    "# dhprebn += (1.0/n)*dbnmeani.broadcast_to(hprebn.shape)\n",
    "\n",
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) -n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('dhprebn',dhprebn, hprebn)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6438b634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "0 tensor(3.6209)\n",
      "10000 tensor(2.4809)\n",
      "20000 tensor(1.8566)\n",
      "30000 tensor(2.3609)\n",
      "40000 tensor(1.9766)\n",
      "50000 tensor(1.9988)\n",
      "60000 tensor(1.9970)\n",
      "70000 tensor(2.5475)\n",
      "80000 tensor(1.9352)\n",
      "90000 tensor(1.7701)\n",
      "100000 tensor(2.4469)\n",
      "110000 tensor(2.3137)\n",
      "120000 tensor(2.1310)\n",
      "130000 tensor(1.9865)\n",
      "140000 tensor(2.3196)\n",
      "150000 tensor(1.9866)\n",
      "160000 tensor(2.3695)\n",
      "170000 tensor(1.9863)\n",
      "180000 tensor(2.2959)\n",
      "190000 tensor(2.7086)\n"
     ]
    }
   ],
   "source": [
    "# Ex 4 : Train using manual steps\n",
    "\n",
    "n_embd = 10 # dimensionality of char embedding vectors\n",
    "n_hidden = 200 # num of hidden neurons\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((vocab_size,n_embd), generator = g)\n",
    "#Layer 1\n",
    "W1 = torch.randn((n_embd*block_size, n_hidden), generator=g) * (5/3) / ((n_embd*block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1 # just to check grad\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# batchNorm params\n",
    "bngain = torch.randn((1,n_hidden), generator =g ) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1,n_hidden))*0.1\n",
    "\n",
    "# non standard init for just check grads as zero vals can mask incorect gradients\n",
    "\n",
    "parameters = [C,W1,b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # total params\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "batch_size = 32\n",
    "n = batch_size # shorter var size for convenice\n",
    "max_steps = 200000\n",
    "lossi = []\n",
    "with torch.no_grad():\n",
    "    for i in range(max_steps):\n",
    "        # construct a minibatch\n",
    "        ix = torch.randint(0, Xtr.shape[0],(batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "        # forward pass, chunked into smaller steps to backward one at a time\n",
    "\n",
    "        emb = C[Xb] # embed char into vectors\n",
    "        embcat = emb.view(emb.shape[0],-1) # concat \n",
    "        # linear layer 1\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre activation\n",
    "        # batch norm layer\n",
    "        bnmean = hprebn.mean(0,keepdim=True)\n",
    "        bnvar = hprebn.var(0,keepdim=True)\n",
    "        bnvar_inv = (bnvar+1e-5)**-0.5\n",
    "        bnraw = (hprebn-bnmean)*bnvar_inv\n",
    "        hpreact = bngain*bnraw + bnbias\n",
    "        # non-linearity\n",
    "        h = torch.tanh(hpreact) # hidden layer tanh\n",
    "        # linear layer 2\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits,Yb)\n",
    "        if i%10000==0:    \n",
    "            print(i, loss)\n",
    "\n",
    "        # pytorch backward pass\n",
    "        for p in parameters:\n",
    "            p.grad=None\n",
    "#         loss.backward()\n",
    "        # loss\n",
    "\n",
    "        dlogits = F.softmax(logits,1)\n",
    "        dlogits[range(n),Yb] -= 1\n",
    "        dlogits /= n\n",
    "\n",
    "        # 2nd layer backprop\n",
    "        # logits = h @ W2 + b2\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0) # broadcasted across rows (replicated vertically)\n",
    "\n",
    "        # h = torch.tanh(hpreact)\n",
    "        dhpreact = (1.0-h**2)*dh\n",
    "\n",
    "        # Batchnorm backprop\n",
    "        dbngain = (bnraw*dhpreact).sum(0,keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0)\n",
    "\n",
    "        dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) -n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "        # hprebn = embcat @ W1 + b1\n",
    "        db1 = dhprebn.sum(0)\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        dembcat = dhprebn @ W1.T\n",
    "\n",
    "        # embcat = emb.view(emb.shape[0],-1)\n",
    "        demb = dembcat.reshape(emb.shape)\n",
    "\n",
    "        # emb = C[Xb]\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k,j]\n",
    "                dC[ix]+=demb[k,j]\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        \n",
    "        # update\n",
    "        lr = 0.1 if i<max_steps/2 else 0.01 # step lr decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            p.data += -lr * grad\n",
    "            \n",
    "#         for p,z in zip(parameters,grads):\n",
    "#             cmp(str(tuple(p.shape)),z,p)\n",
    "            \n",
    "        lossi.append(loss.log().item())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de8501a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # pass the training set\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0],-1)\n",
    "    hpreact = embcat@W1 + b1\n",
    "    # measure mean/std\n",
    "    bnmean = hpreact.mean(0,keepdim=True)\n",
    "    bnvar = hpreact.var(0,keepdim=True, unbiased=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9bf2714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0707921981811523\n",
      "val 2.118593215942383\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr,Ytr),\n",
    "        'val' : (Xdev,Ydev),\n",
    "        'test' : (Xte,Yte),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    hpreact = emb.view(emb.shape[0],-1)@W1 + b1\n",
    "    hpreact = bngain *(hpreact-bnmean)*(bnvar+1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h@W2 + b2\n",
    "    loss = F.cross_entropy(logits,y)\n",
    "    print(split,loss.item())\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "406cee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carman.\n",
      "ambril.\n",
      "khi.\n",
      "mili.\n",
      "taty.\n",
      "skaan.\n",
      "kenleine.\n",
      "farliyat.\n",
      "kaeli.\n",
      "nellara.\n",
      "chaiiv.\n",
      "kaleigh.\n",
      "ham.\n",
      "joce.\n",
      "quint.\n",
      "suline.\n",
      "liven.\n",
      "corathon.\n",
      "jarinix.\n",
      "kaellinsley.\n"
     ]
    }
   ],
   "source": [
    "# Sample from the dataset\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # ...\n",
    "    while True:\n",
    "        \n",
    "        emb = C[torch.tensor([context])] # 1xblock_sizexD\n",
    "        hpreact = emb.view(emb.shape[0],-1)@W1 + b1\n",
    "        hpreact = bngain *(hpreact-bnmean)*(bnvar+1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h@W2 + b2\n",
    "        \n",
    "        # Sample\n",
    "        probs = F.softmax(logits,dim=1)\n",
    "        ix = torch.multinomial(probs,num_samples=1, generator=g).item()\n",
    "        context = context[1:]+[ix]\n",
    "        out.append(ix)\n",
    "        if ix==0:\n",
    "            break\n",
    "            \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e541587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
